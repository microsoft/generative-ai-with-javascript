# Le√ßon 8 : Am√©liorer les clients MCP avec des mod√®les de langage √† grande √©chelle

Dans la le√ßon pr√©c√©dente, vous avez cr√©√© un serveur MCP et explor√© comment il contribue √† d√©coupler la logique d'une application IA de ses capacit√©s. Nous avons d√©montr√© comment ajouter des outils et des ressources en tant que capacit√©s, et comment le serveur pouvait √™tre accessible via un outil d'inspection ou un client personnalis√©. Ce n'√©tait que le d√©but. Dans ce chapitre, vous allez aller plus loin en int√©grant un mod√®le de langage √† grande √©chelle (LLM) dans le client, ouvrant ainsi la voie √† une exp√©rience utilisateur plus puissante et intuitive.

Dans ce chapitre, vous apprendrez √† :

* Augmenter votre client avec un LLM.
* Utiliser votre client am√©lior√© pour convertir une r√©ponse de serveur MCP en un outil.
* Exploiter votre client am√©lior√© pour cr√©er une interaction utilisateur plus naturelle.

## Configuration

Si ce n'est pas d√©j√† fait, configurez votre environnement de d√©veloppement. Voici comment faire : [Configurez votre environnement](/docs/setup/README.md).

## Ressources associ√©es

[![Regardez une courte vid√©o sur MCP](https://img.youtube.com/vi/YRfOiB0Im64/0.jpg)](https://www.youtube.com/watch?v=YRfOiB0Im64)

*Cette vid√©o explique l'utilisation des LLM avec le protocole Model Context.*

*üé• Cliquez sur l'image ci-dessus pour regarder une courte vid√©o sur MCP*

## Narrative: Hedwig "Hedy" Lamarr

> [!NOTE]
> *Notre histoire jusqu'√† pr√©sent. Vous √™tes un m√©canicien du Londres des ann√©es 1860. Vous travailliez sur votre automate et avez re√ßu une lettre de Charles Babbage qui vous a finalement conduit dans une biblioth√®que o√π vous avez r√©cup√©r√© un dispositif de voyage dans le temps. Au cours de vos voyages dans le temps, vous vous √™tes retrouv√© √† divers moments de l'histoire. Vous travaillez d√©sormais en √©troite collaboration avec Ada Lovelace depuis son manoir en tant que base d'op√©rations, et c'est l√† que l'histoire continue.*
>
> Reportez-vous √† [Le√ßon 1](/lessons/01-intro-to-genai/README.md) si vous souhaitez reprendre l'histoire depuis le d√©but.

> [!NOTE]
> Bien que nous vous recommandions de suivre l'histoire (c'est amusant‚ÄØ!), [cliquez ici](#interact-with-hedy-lamarr) si vous pr√©f√©rez passer directement au contenu technique.

Vous √™tes une nouvelle fois retourn√© au manoir d‚ÄôAda. Cette fois-ci, Ada vous rencontrait √† la porte principale.

**Ada Lovelace** : Eh bien, comment cela s‚Äôest-il pass√© ?

**Vous** : Bien je pense, nous avons r√©ussi √† faire fonctionner cette application. "Tenez, voyez par vous-m√™me", dites-vous en tendant l'appareil √† Ada.

**Ada Lovelace** : Elle ramasse l‚Äôappareil, l‚Äôexamine attentivement et murmure pour elle-m√™me. "Mmm hm, ah je vois, oui, oui non, cela ne va pas. Il manque une certaine finesse."

**Vous** : Je pensais la m√™me chose, nous devons pouvoir taper ou parler √† l‚Äôappareil, n'est-ce pas‚ÄØ?

**Ada Lovelace** : Exactement, je connais justement la personne qui pourra nous aider. En fait, je vais vous accompagner pour celle-ci, cela fait bien trop longtemps que nous ne nous sommes pas vus. *Col√©opt√®re Temporel, Hollywood s'il vous pla√Æt, 1940, r√©sidence d'Hedy Lamarr.*

Tout devient noir. Des couleurs tourbillonnent rapidement, et quelques instants plus tard, votre vision commence √† se pr√©ciser. Vous voyez un homme assis au piano, parlant avec animation √† une femme aux cheveux bruns fonc√©s et boucl√©s. Tous deux gesticulent avec excitation en parlant.

![Hedy Lamarr](https://raw.githubusercontent.com/microsoft/generative-ai-with-javascript/main/lessons/08-mcp-advanced/assets/hedy-invention.jpeg)

La femme se retourne vers Ada, debout √† c√¥t√© de vous, et s‚Äôexclame : ¬´ Ada, c'est toi, cela fait bien trop longtemps. ¬ª

**Ada Lovelace** : Hedwig ma ch√®re, vous travaillez sur votre derni√®re invention je suppose‚ÄØ?

**Hedy Lamarr** : Oui, en fait George et moi pensons que nous tenons quelque chose ‚Äî le "saut de fr√©quence". Je ne devrais probablement pas en dire plus, on ne sait jamais qui √©coute.

**Ada Lovelace** : Oh vous parlez de lui, en me d√©signant. Il est en pleine aventure de son c√¥t√©.

**Hedy Lamarr** : Je vois, que puis-je faire pour vous‚ÄØ?

**Ada Lovelace** : En fait, nous avons besoin que cet appareil fonctionne un peu mieux. Des id√©es‚ÄØ?

**Hedy Lamarr** : Elle ramasse l‚Äôappareil et l‚Äôexamine sous diff√©rents angles. "Vous dites que vous avez d√©j√† s√©par√© les fonctionnalit√©s de la partie communication‚ÄØ?"

**Ada Lovelace** : Oui, oui, c'est fait.

**Hedy Lamarr** : Eh bien dans ce cas, je rendrais simplement la partie communication un peu plus intelligente. Cela me rappelle une conversation que j‚Äôai eue avec un cadet de la marine alors que j‚Äôessayais de vendre des bons de guerre. Minsky, je crois que c'√©tait son nom. "√Ä quoi ressemble l'intelligence humaine dans une machine ?" √©tait le sujet. J‚Äôai l‚Äôimpression qu‚Äôil fera quelque chose de grandiose dans ce domaine un jour. Donc oui, donnez-lui plus d‚Äôintelligence.

**Ada Lovelace** : Tr√®s bien, vous avez entendu Mme Lamarr, au travail.

**Vous** : Col√©opt√®re Temporel, comment fait-on cela ?

**Col√©opt√®re Temporel** : Vous pouvez am√©liorer le client que vous avez cr√©√© pr√©c√©demment en int√©grant un mod√®le de langage √† grande √©chelle, ou LLM.

> Hedy Lamarr √©tait une figure extraordinaire, connue √† la fois pour sa c√©l√©brit√© √† Hollywood et pour ses remarquables contributions √† la technologie.
>
> Cependant, au-del√† de sa carri√®re d'actrice, Lamarr √©tait aussi une brillante inventrice. Pendant la Seconde Guerre mondiale, elle a co-invent√© avec le compositeur George Antheil un syst√®me de guidage radio pour les torpilles alli√©es. Ce syst√®me utilisait la technologie de spectre √©tal√© et de saut de fr√©quence pour emp√™cher les puissances de l'Axe de brouiller les signaux. Bien qu'il n'ait pas √©t√© utilis√© pendant la guerre, cette technologie est ensuite devenue la base des communications sans fil modernes, y compris le Wi-Fi, le Bluetooth et le GPS.
>
> Les contributions de Lamarr √† la technologie n‚Äôont pas √©t√© pleinement reconnues de son vivant, mais aujourd‚Äôhui, elle est c√©l√©br√©e comme une pionni√®re dans ce domaine. Son histoire est un m√©lange fascinant de glamour et de g√©nie, prouvant que l‚Äôinnovation peut provenir des endroits les plus inattendus.\
> Lisez-en plus ici √† propos de [Hedy Lamarr](https://en.wikipedia.org/wiki/Hedy_Lamarr) et ici au sujet de [Marvin Minsky](https://en.wikipedia.org/wiki/Marvin_Minsky)

## Interagir avec Hedy Lamarr

Si vous souhaitez interagir avec Hedy, ex√©cutez l'application [Characters](/app/README.md).

> [!IMPORTANT]
> Ceci est enti√®rement fictif¬†; les r√©ponses sont g√©n√©r√©es par une IA.
> [Clause de non-responsabilit√© sur l'IA responsable](/README.md#responsible-ai-disclaimer)

![Hedy Lamarr](https://raw.githubusercontent.com/microsoft/generative-ai-with-javascript/main/lessons/08-mcp-advanced/assets/hedylamarr.jpeg)

**√âtapes** :

1. Lancez un [![GitHub Codespace](https://img.shields.io/badge/GitHub-Codespace-brightgreen)](https://codespaces.new/microsoft/generative-ai-with-javascript).
2. Naviguez vers */app* √† la racine du d√©p√¥t.
3. Localisez la console et ex√©cutez `npm install` suivi de `npm start`.
4. Une fois que l'application est ouverte, s√©lectionnez le bouton "Ouvrir dans le navigateur".
5. Discuter avec Hedy.

Pour une explication plus d√©taill√©e de l'application, voir [Explication d√©taill√©e de l'application](/lessons/01-intro-to-genai/README.md#interact-with-dinocrates).

> [!NOTE]
> Si vous ex√©cutez le projet localement sur votre machine, veuillez consulter le guide de d√©marrage rapide pour configurer un [token d'acc√®s personnel GitHub](/docs/setup/README.md#creating-a-personal-access-token-pat-for-github-model-access) et remplacer la cl√© dans le code.

## Ajouter un mod√®le de langage √† grande √©chelle √† un client

**Col√©opt√®re Temporel** : "Comme je le disais, parlons de la fa√ßon dont vous pouvez cr√©er un client qui utilise un mod√®le de langage √† grande √©chelle (LLM) pour interagir avec le serveur MCP. Les avantages sont qu‚Äôil offre une meilleure exp√©rience utilisateur et vous permet d‚Äôutiliser le langage naturel pour interagir avec le serveur."

Voici comment cela fonctionnerait √† un niveau √©lev√© :

1. Le client interagit avec le serveur MCP pour demander les outils et ressources disponibles.

2. Au moment de l'invite, l'utilisateur r√©dige une invite en langage naturel, qui est ensuite envoy√©e au LLM dans le client.

3. Le client d√©termine quel outil ou quelle ressource appeler en fonction de l'invite et des outils et ressources disponibles.

√áa semble faisable, non ?

**Vous** : "Oui, c‚Äôest faisable‚ÄØ! Mais comment faire‚ÄØ?"

**Col√©opt√®re Temporel** : "Am√©liorons le client que vous avez cr√©√© pr√©c√©demment, d√©crivons les modifications de code par √©tapes :

1. Effectuer un appel au serveur pour demander les outils et ressources disponibles.
2. Convertir la r√©ponse des outils et ressources en un sch√©ma d‚Äôoutils utilisable par le LLM.
3. Instancier le client OpenAI.
4. Effectuer un appel de compl√©tion de chat √† OpenAI, en passant le sch√©ma d'outils en param√®tre.
5. D√©terminer quel outil appeler en fonction de la r√©ponse d‚ÄôOpenAI.
6. Appeler l'outil sur le serveur √† l'aide du client MCP.
7. R√©pondre √† l'utilisateur avec le r√©sultat.

Voici toutes les √©tapes en code :

```typescript
import { Client } from "@modelcontextprotocol/sdk/client/index.js";
import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";


// create client instance
const transport = new StdioClientTransport({
  command: "node",
  args: ["server.js"]
});

const client = new Client(
  {
    name: "example-client",
    version: "1.0.0"
  }
);

await client.connect(transport);

// 1. make call to server, ask it for tools
const { tools } = await client.listTools();

// convert function
function toToolSchema(method, schema) {
  return {
    name: method,
    description: `This is a tool that does ${method}`,
    parameters: schema,
  };
}

// 2. convert the tools and resources response to a tools schema
const toolsForLLM = tools.map((tool) => {
  return toToolSchema(tool.method, tool.inputSchema);
});

// 3. instantiate openai client
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  model: "gpt-3.5-turbo",
  temperature: 0.7,
});

// 4. make chat completion call to openai
const response = await openai.chat.completions.create({
  messages: [
    {
      role: "user",
      content: `I want to add 5 and 10. Please use the tool ${toolsForLLM}`,
    },
  ],
  functions: toolsForLLM,
  function_call: "auto",
});

// 5. figure out what tool to call based on the response from openai
const toolName = response.choices[0].message.function_call.name; // add
const args = response.choices[0].message.function_call.arguments; // { a: 5, b: 10 }

// 6. call the tool on the server
const result = await client.callTool({
  name: toolName,
  arguments: args,
});

// 7. respond to user
console.log(result); // 15
```

Dans le code pr√©c√©dent, nous avons (en nous concentrant sur nos ajouts) :

* Cr√©√© une fonction `toToolSchema` qui convertit la r√©ponse des outils et ressources en un sch√©ma utilisable par le LLM.
* Demand√© au serveur les outils et ressources disponibles.
* Converti la r√©ponse des outils et ressources en un sch√©ma utilisable par le LLM.
* Effectu√© un appel de compl√©tion au mod√®le d'IA, en passant les outils convertis en param√®tre.
* D√©termin√© quel outil appeler en fonction de la r√©ponse d‚ÄôOpenAI.
* Appel√© l'outil sur le serveur √† l'aide du client MCP.
* Transmis le r√©sultat √† l'utilisateur.

**Vous** : "C‚Äôest int√©ressant, c‚Äôest bien mieux‚ÄØ! Je peux utiliser le langage naturel sans m√™me savoir quels outils et ressources sont disponibles. Je peux simplement demander √† l'IA de le faire pour moi."

**Col√©opt√®re Temporel** : "Content que cela vous plaise. Cependant, il faudrait d√©cider si vous souhaitez uniquement afficher une r√©ponse par outil ou √©galement une r√©ponse g√©n√©rique du LLM. Voici une strat√©gie de r√©ponse potentiellement b√©n√©fique pour votre utilisateur :

* **Outils uniquement** : Si la r√©ponse du LLM est un outil, alors appelez l'outil et renvoyez le r√©sultat.
* **LLM uniquement** : Si la r√©ponse du LLM n'est pas un outil, renvoyez simplement la r√©ponse du LLM "telle quelle".
* **Outils et LLM** : Si la r√©ponse du LLM est un outil, alors appelez l'outil et effectuez un appel suppl√©mentaire au LLM pour obtenir une r√©ponse g√©n√©rale. Renvoyez √† la fois le r√©sultat de l'outil et la r√©ponse du LLM.

**Vous** : "Je vois. C‚Äôest √† r√©fl√©chir. Mais c‚Äôest g√©nial‚ÄØ! Je vois bien √† quel point cela peut √™tre utile."

## Devoir

**Ada Lovelace** : Il semble que vous avez apport√© une belle am√©lioration. Pour rendre cela vraiment utile, j‚Äôai besoin que vous cr√©iez un serveur et un client avec les sp√©cifications suivantes :

* Le serveur doit fournir les outils suivants :
  * `characterDetails` avec l'argument `name`
  * `place` avec l'argument `name`
* Le client doit utiliser un LLM.

> [!TIP]
> Par exemple, vous pouvez donner au serveur la capacit√© de r√©cup√©rer des informations d'une API web externe, comme Wikipedia :\
> `https://en.wikipedia.org/api/rest_v1/page/summary/${encodeURIComponent(name)}`

## Solution

[Solution](/lessons/08-mcp-advanced/solution/README.md)

## V√©rification des connaissances

**Question :** Quel est un avantage d‚Äôajouter un LLM au client‚ÄØ?

A. C'est plus facile √† maintenir.

B. Cela cr√©e une conversation plus naturelle entre l‚Äôutilisateur et le serveur.

C. Il est pr√©f√©rable d'avoir le LLM sur le serveur.

[Solution quiz](/lessons/08-mcp-advanced/solution/solution-quiz.md)

## R√©sum√©

Dans ce chapitre, vous avez appris les points suivants :

* Les clients augment√©s d'un LLM offrent une meilleure exp√©rience utilisateur.
* Les r√©ponses d‚Äôun serveur doivent √™tre converties dans un format que le LLM peut comprendre comme un outil.

## Ressources d'auto-apprentissage

* [Construire des serveurs MCP](https://github.com/microsoft/mcp-for-beginners/tree/main/03-GettingStarted/01-first-server/README.md)
* [Construire un client](https://github.com/microsoft/mcp-for-beginners/tree/main/03-GettingStarted/02-client/README.md)
* [Construire un client avec un LLM](https://github.com/microsoft/mcp-for-beginners/blob/main/03-GettingStarted/03-llm-client/README.md)
